{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSiA 431 - Big Data - Homework 3 - Iteration 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kristiyan Dimitrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, weekofyear, year, month, window, count, lag, first, last, desc \n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Problem 3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_path, header = True , inferSchema = True, timestampFormat='YYYY-MM-DD HH:MM:SS a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together in a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "alpha = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the EWMA function, which takes a list of values and an alpha parameters and calculates the exponentially weighted average\n",
    "## Testing EWMA with a range of values\n",
    "def ewma_lst(alpha, lst):\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    for ii in range(len(lst)):\n",
    "        res += alpha * ( (1-alpha)**ii ) * lst[ii]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDF\n",
    "ewma = spark.udf.register(\"ewma_lst\", ewma_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only data after 2008, before that is not relevant\n",
    "df_2008 = df.filter(df.time_stamp >= '2008-01-01 00:00:00')\n",
    "\n",
    "# Dropping 'direction' column\n",
    "df_2008 = df_2008.drop('direction')\n",
    "\n",
    "# Taking first chunk of data; NEED TO PARAMETRIZE WITH YEAR & MONTH LATER <------------------ <------------------ <------------------ <------------------ <------------------\n",
    "df_1 = df_2008.filter(df_2008.time_stamp <= '2008-06-01 00:00:00')\n",
    "\n",
    "# Let's take a subset of columns for ease\n",
    "df_subset = df_1.select(['bar_num', 'profit', 'trade_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the max bar value across all trades\n",
    "max_bar_per_trade = df_1.groupBy(col(\"trade_id\")).agg({\"bar_num\": \"max\"}).alias('max_bar_num')\n",
    "max_bar = max_bar_per_trade.agg({\"max(bar_num)\": \"max\"}).collect()[0][0]\n",
    "max_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema for empty dataframe, which will hold all the calculated ewma profits\n",
    "schema = StructType([StructField('trade_id', LongType(), False),\n",
    "                     StructField('profit_ewma', DoubleType(), False), \n",
    "                     StructField('bar_num', LongType(), True)])\n",
    "\n",
    "results = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[bar_num: int, profit: int, trade_id: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this for loop is to calculate a feature based on profit\n",
    "\n",
    "for ii in range(11, max_bar): # For bars 1, 2, 3, ... 10, we don't need to do anything; So, when we do left join, those feature values for bars 1-10 should be null\n",
    "    \n",
    "#     print(f'Start {ii}')\n",
    "    \n",
    "    if ii % 10 == 0: # This means we are in the situation of taking bar 20, 30, 40, etc.\n",
    "        bars_to_take = ii - 10 # For bar 20, we want bars 10 and below; for bar 30, we want bars 20 and below...\n",
    "    else:\n",
    "        bars_to_take = ii - ii%10 # E.g. if we are at bar 33, we want bars 33 - 3 = 30 and below\n",
    "    \n",
    "    # Taking only the part of the dataset, which contains the subset of bars we are interested in\n",
    "    df_filtered = df_subset.filter(f'bar_num <= {bars_to_take}')\n",
    "        \n",
    "    # Collecting all the profits for a given trade_id in one place (list with its corresponding trade_id)\n",
    "    df_intermediate = df_filtered.groupby('trade_id').agg(F.collect_list('profit'))\n",
    "        \n",
    "    # Apply the UDF EWMA function to the collected list of profits\n",
    "    df_ewma = df_intermediate.select('trade_id', ewma(F.lit(alpha), col(\"collect_list(profit)\")))\n",
    "        \n",
    "    # Adding the bar_num we are currently at, so we can properly join later\n",
    "    final_df = df_ewma.withColumn('bar_num', lit(ii))\n",
    "        \n",
    "    results = results.union(final_df)   \n",
    "    \n",
    "#     print(f'End {ii}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+\n",
      "|trade_id|        profit_ewma|bar_num|\n",
      "+--------+-------------------+-------+\n",
      "|    9900| 14.230217728000003|     11|\n",
      "|    9852|-45.088219033600005|     11|\n",
      "|   10081|  42.92491776000001|     11|\n",
      "|    9879| -64.16702259200002|     11|\n",
      "|   10121|-16.785674240000006|     11|\n",
      "|    9946|  9.038351359999998|     11|\n",
      "|   10032| 15.545000038400003|     11|\n",
      "|    9775| 28.437628416000003|     11|\n",
      "|    9914|  93.61254041600002|     11|\n",
      "|   10090|      -37.236989952|     11|\n",
      "+--------+-------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.filter(results.trade_id == 9900).show() # Tried to let this run for ~ 10 minutes, but it didn't finish; aborting and hoping this works on distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to join the original dataframe with the results i.e. to add the profit_ewma feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+\n",
      "|         time_stamp|bar_num|profit|var12|var13|var14|var15|var16|var17|var18|var23|var24|var25|var26|var27|var28|var34|var35|var36|var37|var38|var45|var46|var47|var48|var56|var57|var58|var67|var68|var78|trade_id|\n",
      "+-------------------+-------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+\n",
      "|2008-04-10 04:19:00|    120|  -131|    9|    7|    7|    7|    7|    9|    9|    6|    6|    6|    7|    7|    7|    6|    6|    7|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:18:00|    119|   -97|    3|    2|    2|    2|    2|    3|    3|    1|    1|    1|    1|    2|    2|    1|    1|    2|    3|    3|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:17:00|    118|  -124|    9|    9|    9|    9|    9|    9|    9|    6|    6|    6|    6|    7|    7|    6|    6|    7|    9|    9|    8|    8|    9|    9|    8|    9|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:16:00|    117|  -102|    3|    3|    3|    3|    3|    3|    3|    1|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:15:00|    116|  -139|    5|    7|    7|    7|    7|    7|    7|    6|    6|    6|    6|    6|    6|    6|    6|    7|    7|    7|    9|    9|    9|    9|    9|    9|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:14:00|    115|   -96|   -2|   -3|    6|    6|    6|    6|    6|    4|    4|    4|    4|    4|    4|    4|    1|    1|    1|    1|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:13:00|    114|   -86|   -9|   -9|    1|    1|    1|    1|    1|    4|    1|    1|    1|    1|    1|    1|    1|    1|    2|    2|    3|    3|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:12:00|    113|   -92|   -3|   -8|    4|    4|    4|    4|    4|    4|    4|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:11:00|    112|   -82|   -7|   -5|    1|    1|    1|    2|    2|    1|    1|    1|    1|    1|    1|    1|    1|    3|    3|    3|    3|    3|    8|    8|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:10:00|    111|  -141|   -2|    6|    6|    6|    6|    7|    7|    6|    6|    6|    6|    6|    6|    6|    6|    7|    9|    9|    9|    9|    9|    9|    9|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:09:00|    110|   -78|   -3|   -8|    4|    4|    4|    4|    4|    1|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    3|    8|    8|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:08:00|    109|   -67|   -8|   -5|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    3|    3|    3|    3|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:07:00|    108|   -68|   -5|    1|    1|    1|    1|    1|    1|    1|    1|    2|    2|    2|    2|    2|    3|    3|    3|    3|    3|    3|    3|    3|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:06:00|    107|  -126|    6|    4|    4|    4|    4|    6|    6|    1|    1|    3|    3|    8|    8|    3|    8|    8|    8|    8|    8|    8|    8|    8|    5|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:05:00|    106|   -96|   -5|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    3|    3|    3|    3|    3|    3|    8|    8|    8|    8|   -4|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:04:00|    105|  -116|    1|    1|    1|    1|    2|    2|    2|    2|    3|    3|    3|    3|    3|    3|    3|    8|    8|    8|    8|    8|    8|    8|   -1|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:03:00|    104|  -122|    2|    2|    2|    2|    3|    3|    3|    3|    3|    3|    3|    3|    3|    3|    3|    8|    8|    8|    8|    8|    8|    8|   -1|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:02:00|    103|  -183|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|   -1|    9|   -2|    6|    6|    9853|\n",
      "|2008-04-10 04:01:00|    102|  -162|    3|    8|    8|    8|    8|    8|    8|    8|    8|    8|    8|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|   -1|    9|   -3|    6|    6|    9853|\n",
      "|2008-04-10 04:00:00|    101|  -164|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|   -1|    9|   -3|    6|    6|    9853|\n",
      "+-------------------+-------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_new_column = df_1.join(results, on = ['trade_id', 'bar_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4731.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 20437 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7db555b1f789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_with_new_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4731.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 20437 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df_with_new_column.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
