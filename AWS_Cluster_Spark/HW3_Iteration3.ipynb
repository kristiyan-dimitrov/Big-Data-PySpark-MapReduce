{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSiA 431 - Big Data - Homework 3 - Iteration 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kristiyan Dimitrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, weekofyear, year, month, window, count, lag, first, last, desc \n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Problem 3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_path, header = True , inferSchema = True, timestampFormat='YYYY-MM-DD HH:MM:SS a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together in a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "alpha = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the EWMA function, which takes a list of values and an alpha parameters and calculates the exponentially weighted average\n",
    "## Testing EWMA with a range of values\n",
    "def ewma_lst(alpha, lst):\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    for ii in range(len(lst)):\n",
    "        res += alpha * ( (1-alpha)**ii ) * lst[ii]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDF\n",
    "ewma = spark.udf.register(\"ewma_lst\", ewma_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only data after 2008, before that is not relevant\n",
    "df_2008 = df.filter(df.time_stamp >= '2008-01-01 00:00:00')\n",
    "\n",
    "# Dropping 'direction' column\n",
    "df_2008 = df_2008.drop('direction')\n",
    "\n",
    "# Taking first chunk of data; NEED TO PARAMETRIZE WITH YEAR & MONTH LATER <------------------ <------------------ <------------------ <------------------ <------------------\n",
    "df_1 = df_2008.filter(df_2008.time_stamp <= '2008-06-01 00:00:00')\n",
    "\n",
    "# Let's take a subset of columns for ease\n",
    "df_subset = df_1.select(['bar_num', 'profit', 'trade_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the max bar value across all trades\n",
    "max_bar_per_trade = df_1.groupBy(col(\"trade_id\")).agg({\"bar_num\": \"max\"}).alias('max_bar_num')\n",
    "max_bar = max_bar_per_trade.agg({\"max(bar_num)\": \"max\"}).collect()[0][0]\n",
    "max_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema for empty dataframe, which will hold all the calculated ewma profits\n",
    "schema = StructType([StructField('trade_id', LongType(), False),\n",
    "                     StructField('profit_ewma', DoubleType(), False), \n",
    "                     StructField('bar_num', LongType(), True)])\n",
    "\n",
    "results = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[bar_num: int, profit: int, trade_id: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this for loop is to calculate a feature based on profit\n",
    "\n",
    "for ii in range(11, max_bar): # For bars 1, 2, 3, ... 10, we don't need to do anything; So, when we do left join, those feature values for bars 1-10 should be null\n",
    "    \n",
    "#     print(f'Start {ii}')\n",
    "    \n",
    "    if ii % 10 == 0: # This means we are in the situation of taking bar 20, 30, 40, etc.\n",
    "        bars_to_take = ii - 10 # For bar 20, we want bars 10 and below; for bar 30, we want bars 20 and below...\n",
    "    else:\n",
    "        bars_to_take = ii - ii%10 # E.g. if we are at bar 33, we want bars 33 - 3 = 30 and below\n",
    "    \n",
    "    # Taking only the part of the dataset, which contains the subset of bars we are interested in\n",
    "    df_filtered = df_subset.filter(f'bar_num <= {bars_to_take}')\n",
    "        \n",
    "    # Collecting all the profits for a given trade_id in one place (list with its corresponding trade_id)\n",
    "    df_intermediate = df_filtered.groupby('trade_id').agg(F.collect_list('profit'))\n",
    "        \n",
    "    # Apply the UDF EWMA function to the collected list of profits\n",
    "    df_ewma = df_intermediate.select('trade_id', ewma(F.lit(alpha), col(\"collect_list(profit)\")))\n",
    "        \n",
    "    # Adding the bar_num we are currently at, so we can properly join later\n",
    "    final_df = df_ewma.withColumn('bar_num', lit(ii))\n",
    "        \n",
    "    results = results.union(final_df)   \n",
    "    \n",
    "#     print(f'End {ii}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could generate the same ewma feature for all the val## features as well (I'd just leave out the little bit of if-else logic in the beginning and have bars_to_take=ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+\n",
      "|trade_id|        profit_ewma|bar_num|\n",
      "+--------+-------------------+-------+\n",
      "|    9900| 14.230217728000003|     11|\n",
      "|    9852|-45.088219033600005|     11|\n",
      "|   10081|  42.92491776000001|     11|\n",
      "|    9879| -64.16702259200002|     11|\n",
      "|   10121|-16.785674240000006|     11|\n",
      "|    9946|  9.038351359999998|     11|\n",
      "|   10032| 15.545000038400003|     11|\n",
      "|    9775| 28.437628416000003|     11|\n",
      "|    9914|  93.61254041600002|     11|\n",
      "|   10090|      -37.236989952|     11|\n",
      "+--------+-------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.filter(results.trade_id == 9900).show() # Tried to let this run for ~ 10 minutes, but it didn't finish; aborting and hoping this works on distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to join the original dataframe with the results i.e. to add the profit_ewma feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+\n",
      "|         time_stamp|bar_num|profit|var12|var13|var14|var15|var16|var17|var18|var23|var24|var25|var26|var27|var28|var34|var35|var36|var37|var38|var45|var46|var47|var48|var56|var57|var58|var67|var68|var78|trade_id|\n",
      "+-------------------+-------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+\n",
      "|2008-04-10 04:19:00|    120|  -131|    9|    7|    7|    7|    7|    9|    9|    6|    6|    6|    7|    7|    7|    6|    6|    7|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:18:00|    119|   -97|    3|    2|    2|    2|    2|    3|    3|    1|    1|    1|    1|    2|    2|    1|    1|    2|    3|    3|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:17:00|    118|  -124|    9|    9|    9|    9|    9|    9|    9|    6|    6|    6|    6|    7|    7|    6|    6|    7|    9|    9|    8|    8|    9|    9|    8|    9|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:16:00|    117|  -102|    3|    3|    3|    3|    3|    3|    3|    1|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:15:00|    116|  -139|    5|    7|    7|    7|    7|    7|    7|    6|    6|    6|    6|    6|    6|    6|    6|    7|    7|    7|    9|    9|    9|    9|    9|    9|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:14:00|    115|   -96|   -2|   -3|    6|    6|    6|    6|    6|    4|    4|    4|    4|    4|    4|    4|    1|    1|    1|    1|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:13:00|    114|   -86|   -9|   -9|    1|    1|    1|    1|    1|    4|    1|    1|    1|    1|    1|    1|    1|    1|    2|    2|    3|    3|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:12:00|    113|   -92|   -3|   -8|    4|    4|    4|    4|    4|    4|    4|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    8|    8|    8|    8|    8|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:11:00|    112|   -82|   -7|   -5|    1|    1|    1|    2|    2|    1|    1|    1|    1|    1|    1|    1|    1|    3|    3|    3|    3|    3|    8|    8|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:10:00|    111|  -141|   -2|    6|    6|    6|    6|    7|    7|    6|    6|    6|    6|    6|    6|    6|    6|    7|    9|    9|    9|    9|    9|    9|    9|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:09:00|    110|   -78|   -3|   -8|    4|    4|    4|    4|    4|    1|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    3|    8|    8|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:08:00|    109|   -67|   -8|   -5|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    3|    3|    3|    3|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:07:00|    108|   -68|   -5|    1|    1|    1|    1|    1|    1|    1|    1|    2|    2|    2|    2|    2|    3|    3|    3|    3|    3|    3|    3|    3|    8|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:06:00|    107|  -126|    6|    4|    4|    4|    4|    6|    6|    1|    1|    3|    3|    8|    8|    3|    8|    8|    8|    8|    8|    8|    8|    8|    5|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:05:00|    106|   -96|   -5|    1|    1|    1|    1|    1|    1|    1|    2|    3|    3|    3|    3|    3|    3|    3|    3|    3|    8|    8|    8|    8|   -4|   -4|    8|   -1|    6|    4|    9853|\n",
      "|2008-04-10 04:04:00|    105|  -116|    1|    1|    1|    1|    2|    2|    2|    2|    3|    3|    3|    3|    3|    3|    3|    8|    8|    8|    8|    8|    8|    8|   -1|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:03:00|    104|  -122|    2|    2|    2|    2|    3|    3|    3|    3|    3|    3|    3|    3|    3|    3|    3|    8|    8|    8|    8|    8|    8|    8|   -1|   -1|    9|   -1|    6|    6|    9853|\n",
      "|2008-04-10 04:02:00|    103|  -183|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|   -1|    9|   -2|    6|    6|    9853|\n",
      "|2008-04-10 04:01:00|    102|  -162|    3|    8|    8|    8|    8|    8|    8|    8|    8|    8|    8|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|   -1|    9|   -3|    6|    6|    9853|\n",
      "|2008-04-10 04:00:00|    101|  -164|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|    9|   -1|   -1|    9|   -3|    6|    6|    9853|\n",
      "+-------------------+-------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_with_new_column = df_1.join(results, on = ['trade_id', 'bar_num']) # This gave an error on my laptop which I hope won't occur on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming the join works, now I need to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The name of the new column is: profit_ewma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that this variable - profit_ewma - should be the same for all 10 bars in a given set of 10 e.g. the values for trade 9853 for bars 11-20 should be hte same, because they are all calculated based on bars 1-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore, I am respecting the assignment condition that profits for bars 11-20 not be available in predicting profits for these bars (when there is the same value, then they are not influencing the prediction and only the features influence the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_with_new_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1a51e240f74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_with_new_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_stamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bar_num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trade_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_with_new_column' is not defined"
     ]
    }
   ],
   "source": [
    "df_features = df_with_new_column.drop('time_stamp', 'bar_num', 'trade_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9c16c227424d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Making sure profit_ewma is a float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"profit_ewma\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"profit_ewma\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Making sure profit_ewma is a float\n",
    "df_features = df_features.withColumn(\"profit_ewma\", df_features[\"profit_ewma\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since all the variables are numeric I don't need Indexers & one-hot encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['profit_ewma','var12','var13','var14','var15','var16','var17','var18','var23','var24','var25','var26','var27','var28','var34','var35','var36','var37','var38','var45','var46','var47','var48','var56','var57','var58','var67','var68','var78'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol = 'profit', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [vectorAssembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b00780f079ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Applying model to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "model = pipeline.fit(df_features)\n",
    "\n",
    "# Applying model to data (This is applying the model to the training data; for the homework I should apply to month 7 in 2008)\n",
    "predictions = model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are supposed to be the two columns needed for calculating MAPE\n",
    "label_and_prediction = predictions.select('profit', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add a small value to profit column to avoid dividing by 0\n",
    "label_and_prediction = label_and_prediction.withColumn('profit', df.profit + 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE = label_and_prediction.withColumn('percent_difference', expr(\"F.abs(profit - prediction)/profit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_MAPE = MAPE.select(F.mean(col('percent_difference')).alias('mean')).collect()\n",
    "\n",
    "mean = avg_MAPE[0]['mean']\n",
    "print(mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
